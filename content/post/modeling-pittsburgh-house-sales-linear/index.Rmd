---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "Modeling Pittsburgh House Sales Linear"
subtitle: ""
summary: ""
authors: [Conor Tompkins]
tags: [R, Housing, Allegheny County]
categories: [R, Housing, Allegheny County]
date: 2019-01-13
lastmod: 2020-09-06
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE, 
                      warning = FALSE)
```
In this post I will be modeling house (land parcel) sales in Pittsburgh. The data is from the WPRDC's [Parcels n'at](http://tools.wprdc.org/parcels-n-at/#) dashboard.

The goal is to use linear modeling to predict the sale price of a house using features of the house and the property. 

This code sets up the environment and loads the libraries I will use.
```{r}
#load libraries
library(tidyverse)
library(scales)
library(caret)
library(broom)
library(modelr)
library(rsample)
library(janitor)
library(vroom)

#set up environment
options(scipen = 999, digits = 5)

theme_set(theme_bw())
```

This reads the data and engineers some features.
```{r}
#read in data
df <- vroom("data/assessments.csv", col_types = cols(.default = "c")) %>% 
  clean_names() %>% 
  mutate(across(.cols = c(saleprice, finishedlivingarea, lotarea, yearblt,
                          bedrooms, fullbaths, halfbaths), parse_number))

#glimpse(df)

# df %>% 
#   select(saleprice, finishedlivingarea, lotarea, yearblt, bedrooms, fullbaths, halfbaths) %>% 
#   glimpse()
# 
# df %>% 
#   select(contains("muni"))
```

```{r}
df <- df %>% 
  mutate(munidesc = str_replace(munidesc, " - PITTSBURGH", "")) %>% 
  mutate(finishedlivingarea_log10 = log10(finishedlivingarea),
         lotarea_log10 = log10(lotarea),
         saleprice_log10 = log10(saleprice)) %>% 
  select(parid, classdesc, munidesc, schooldesc, neighdesc, taxdesc,
         usedesc, homesteadflag, farmsteadflag, styledesc,
         yearblt, extfinish_desc, roofdesc,  basementdesc,
         gradedesc, conditiondesc, stories, totalrooms, bedrooms,
         fullbaths, halfbaths, heatingcoolingdesc, fireplaces, 
         bsmtgarage, finishedlivingarea, finishedlivingarea_log10,
         lotarea, lotarea_log10, saledate,
         saleprice, saleprice_log10)

#create grade vectors
grades_standard <- c("average -", "average", "average +",
                     "good -", "good", "good +",
                     "very good -", "very good", "very good +")

grades_below_average_or_worse <- c("poor -", "poor", "poor +",
                                   "below average -", "below average", "below average +")

grades_excellent_or_better <- c("excellent -", "excellent", "excellent +",
                                "highest cost -", "highest cost", "highest cost +")

#subset data and engineer features
df <- df %>% 
  filter(classdesc == "RESIDENTIAL",
         saleprice > 100,
         str_detect(munidesc, "Ward"),
         finishedlivingarea > 0,
         lotarea > 0) %>% 
  select(parid, munidesc, schooldesc, neighdesc, taxdesc,
         usedesc, homesteadflag, farmsteadflag, styledesc,
         yearblt, extfinish_desc, roofdesc,  basementdesc, 
         heatingcoolingdesc, gradedesc, conditiondesc, stories, 
         totalrooms, bedrooms, fullbaths, halfbaths, fireplaces, 
         bsmtgarage, finishedlivingarea_log10, lotarea_log10, 
         saleprice_log10, saledate) %>% 
  mutate(usedesc = fct_lump(usedesc, n = 5),
         styledesc = fct_lump(styledesc, n = 10),
         #clean up and condense gradedesc
         gradedesc = str_to_lower(gradedesc),
         gradedesc = case_when(gradedesc %in% grades_below_average_or_worse ~ "below average + or worse",
                                    gradedesc %in% grades_excellent_or_better ~ "excellent - or better",
                                    gradedesc %in% grades_standard ~ gradedesc),
         gradedesc = fct_relevel(gradedesc, c("below average + or worse", "average -", "average", "average +",
                                                        "good -", "good", "good +",
                                                        "very good -", "very good", "very good +", "excellent - or better")))

#replace missing character rows with "missing", change character columns to factor
df <- df %>% 
  mutate_if(is.character, replace_na, "missing") %>% 
  mutate_if(is.character, as.factor)

#select response and features
df <- df %>% 
  select(munidesc, usedesc, styledesc, conditiondesc, gradedesc,
         finishedlivingarea_log10, lotarea_log10, yearblt, bedrooms, 
         fullbaths, halfbaths, saleprice_log10) %>% 
  na.omit()

#view data
glimpse(df)
```

As shown in the data above, the model uses the following features to predict sale price:

* municipality name
* primary use of the parcel
* style of building
* condition of the structure
* grade of construction
* living area in square feet
* lot area in square feet
* year the house was built
* number of bedrooms
* number of full baths
* number of half-baths

This code sets up the data for cross validation.
```{r}
#create initial split object
df_split <- initial_split(df, prop = .75)

#extract training dataframe
training_data_full <- training(df_split)

#extract testing dataframe
testing_data <- testing(df_split)

#find dimensions of training_data_full and testing_data
dim(training_data_full)
dim(testing_data)
```
This code divides the data into training and testing sets.
```{r}
set.seed(42)

#prep the df with the cross validation partitions
cv_split <- vfold_cv(training_data_full, v = 5)

cv_data <- cv_split %>% 
  mutate(
    #extract train dataframe for each split
    train = map(splits, ~training(.x)), 
    #extract validate dataframe for each split
    validate = map(splits, ~testing(.x))
  )

#view df
cv_data
```

This builds the model to predict house sale price.
```{r}
#build model using the train data for each fold of the cross validation
cv_models_lm <- cv_data %>% 
  mutate(model = map(train, ~lm(formula = saleprice_log10 ~ ., data = .x)))

cv_models_lm
#problem with factors split across training/validation
#https://stats.stackexchange.com/questions/235764/new-factors-levels-not-present-in-training-data
```

This is where I begin to calculate metrics to judge how well my model is doing.
```{r}
cv_prep_lm <- cv_models_lm %>% 
  mutate(
    #extract actual sale price for the records in the validate dataframes
    validate_actual = map(validate, ~.x$saleprice_log10),
    #predict response variable for each validate set using its corresponding model
    validate_predicted = map2(.x = model, .y = validate, ~predict(.x, .y))
  )

#View data
cv_prep_lm
```

```{r}
#calculate fit metrics for each validate fold       
cv_eval_lm <- cv_prep_lm %>% 
  mutate(validate_rmse = map2_dbl(model, validate, modelr::rmse),
         validate_mae = map2_dbl(model, validate, modelr::mae))

cv_eval_lm <- cv_eval_lm %>% 
  mutate(fit = map(model, ~glance(.x))) %>% 
  unnest(fit)
```
```{r}
#view data
cv_eval_lm %>% 
  select(id, validate_mae, validate_rmse, adj.r.squared)
```

Finally, this calculates how well the model did on the validation set.
```{r}
#summarize fit metrics on cross-validated dfs
cv_eval_lm %>% 
  select(validate_mae, validate_rmse, adj.r.squared) %>% 
  summarize_all(mean)
```
```{r}
#fit model on full training set
train_df <- cv_data %>% 
  select(train) %>% 
  unnest(train)

model_train <- lm(formula = saleprice_log10 ~ ., data = train_df)

model_train %>% 
  glance()
```

This is the RMSE on the training set
```{r}
#calculate rmse on training set
rmse(model_train, train_df)
```

```{r eval=FALSE, include=FALSE}
#create model object
#model <- lm(saleprice_asmt_log10 ~ ., data = train_data_small)

#tidy model
model_train %>% 
  tidy() %>% 
  filter(term != "(Intercept)") %>% 
  arrange(-estimate)

#10th ward is the base factor for muni_desc term
model_train %>% 
  tidy() %>% 
  filter(str_detect(term, "10th"))
```

This shows the impact each term of the model has on the response variable. This is for the training data.
```{r fig.height=12, fig.width=6}
#visualize estimates for terms
model_train %>% 
  tidy() %>% 
  filter(term != "(Intercept)") %>% 
  mutate(term = fct_reorder(term, estimate)) %>% 
  ggplot(aes(term, estimate)) +
  geom_hline(yintercept = 0, linetype = 2, color = "red") +
  geom_point() +
  coord_flip()
```

Next, I apply the model to the testing data to see how the model does out-of-sample.
```{r}
#create dfs for train_data_small and validate_data
#train_data_small <- cv_prep_lm %>% 
#  unnest(train) %>% 
#  select(-id)

validate_df <- cv_prep_lm %>% 
  select(validate) %>% 
  unnest()
```

This creates the augmented dataframe and plots the actual price vs. the fitted price.
```{r}
#visualize model on validate data
augment_validate <- augment(model_train, newdata = validate_df) %>% 
  mutate(.resid = saleprice_log10 - .fitted)

#actual vs. fitted
cv_prep_lm %>% 
  unnest(validate_actual, validate_predicted) %>% 
  ggplot(aes(validate_actual, validate_predicted)) +
  geom_abline() +
  stat_density_2d(aes(fill = stat(level)), geom = "polygon") +
  geom_smooth(method = "lm") +
  scale_x_continuous(limits = c(2, 7)) +
  scale_y_continuous(limits = c(2, 7)) +
  coord_equal() +
  scale_fill_viridis_c()
```

This distribution shows that the model overestimates the prices on many houses.
```{r}
#distribution of residuals
augment_validate %>% 
  ggplot(aes(.resid)) +
  geom_density() +
  geom_vline(xintercept = 0, color = "red", linetype = 2)
```

This shows that the residuals are correlated with the actual price, which indicates that the model is failing to account for some dynamic in the sale process.
```{r}
#sale price vs. residuals
augment_validate %>% 
  ggplot(aes(.resid, saleprice_log10)) +
  stat_density_2d(aes(fill = stat(level)), geom = "polygon") +
  geom_vline(xintercept = 0, color = "red", linetype = 2) +
  scale_fill_viridis_c()
```

This calculates how well the model predicted sale price on out-of-sample testing data.
```{r}
#calculate fit of model on test data
rmse(model_train, validate_df)
mae(model_train, validate_df)
rsquare(model_train, validate_df)
```